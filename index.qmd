---
title: "L'intelligence d'affaires chez Co-operators"
lang: fr
subtitle: "Présentation dans le cadre du cours « Applications probabilistes des risques actuariels »"
author:
  name: Francis Duval, Chercheur Scientifique | Recherche et Innovation Analytique
  email: francis_duval@cooperators.ca
date: 2025-10-07
date-format: long
format: 
  revealjs:
    template-partials:
      - title-slide.html
    theme: [default, styles.scss]
    width: 1280
    height: 720
    chalkboard: true
    transition: slide
    logo: images/logo_blue_2.png
    footer: '[https://github.com/francisduval/presentation_uqam_3_decembre_2024](https://github.com/francisduval/presentation_uqam_3_decembre_2024)'
    slide-number: c/t
    code-block-height: 500px
editor: 
  markdown: 
    wrap: 72
include-after-body: add-custom-footer.html
---

## Mon parcours

### Académique

::: columns

::: {.column width='33%'}
[Baccalauréat]{.three-green-blocks}  

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- **Actuariat**
- 2 stages chez Intact
- Session à l'étranger à Lyon
:::
:::

::: {.column width='33%'}
[Maitrise]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- **Mathématiques actuarielles et financières**
- Mémoire: Techniques de gradient boosting pour la modélisation des réserves individuelles en assurance non-vie
- Stage de recherche Mitacs chez Desjardins
:::
:::

::: {.column width='33%'}
[Doctorat]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- **Mathématiques**
- Thèse: Modélisation des sinistres en assurance automobile avec l’utilisation de données télématiques : approches d’apprentissage automatique en classification et régression de comptage
- Chaire Co-operators en analyse des risques actuariels
:::
:::

:::

### Professionel

- Chercheur scientifique dans l'équipe « Recherche et Innovation Analytique » depuis septembre 2023.

::: notes
- Baccalauréat en Actuariat, où j'ai eu 2 stages en assurance de dommages chez Intact.
- Session à l'étranger à l'Institut de Science Financière et d'Assurance (ISFA).

- Ensuite, j'ai fait une maitrise en mathématiques actuarielles et financière, toujours à l'UQAM.
- J'ai eu la chance durant ma maitrise de faire un stage de recherche Mitacs chez Desjardins. Mitacs est un organisme de recherche sans but lucratif qui offre des bourses de recherche, notamment pour des stages en entreprise.
- Durant ce stage chez Desjardins, j'ai développé un modèle d'évaluation des réserves en utilisant un algorithme de gradient boosting. C'est un modèle de réserve au niveau individuel, c'est-à-dire que contrairement à des modèles comme Chain-Ladder, on a essayer de prédire un montant pour chaque réclamation, de manière individuelle.
- Ce qui était bien avec ce stage, c'est que le modèle que j'ai développé a servi pour mon mémoire de maitrise, et j'ai même publié un article avec Mathieu Pigeon. Donc c'est faire d'une pierre deux coups.

- Suite à ma maitrise, j'ai eu une offre de Jean-Philippe et Mathieu Pigeon pour continuer mes études au sein de la Chaire CARA, qui venait tout juste d'être mise sur pieds. Comme j'avais aimé mon expérience de recherche à la maitrise, et que je trouvais que l'UQAM offrait une solide expertise en assurance de dommages avec des professeurs comme Jean-Philippe et Mathieu en plus du partenariat avec Co-operators, j'ai décidé de commencer un doctorat. De plus, le financement était excellent.
- Dans le doctorat, il y a quelques cours au début, mais le but est surtout de faire de la recherche. J'ai fait ce qu'on appelle une thèse par article, qui consiste en 3 articles. Les articles sont ensuite mis ensemble pour constituer la thèse.
- Ma thèse portait sur la modélisation des sinistres en assurance automobile avec des méthodes de machine learning utilisant des données télématiques.

- Et depuis maintenant 2 ans, je suis chercheur scientifique dans l'équipe de Recherche et Innovation Analytique chez Co-operators, où j'ai déjà eu l'occasion de travailler sur plusieurs projets.
:::


# 1. Introduction à l'assurance de dommages, l'intelligence d'affaires, et Co-operators

## Assurance de dommage

- Aussi appelée « assurance IARD » (Incendie, Accidents et Risques Divers) c'est un type d'assurance qui protège les **individus** et les **entreprises** contre les pertes financières liées à leurs **biens** et à leur **responsabilité** pour les dommages qu'ils peuvent causer à autrui.
- En anglais, **Property & Casualty (P&C) insurance** (property = « biens », casualty = « responsabilité »).

<div style='height:30px;'></div>

::: columns

::: {.column width='48%'}
[Biens]{.two-blue-blocks}  

<div style='height:70px;'></div>

::: {.ul-font-size-medium}
- Assurance habitation: protège contre les incendies, vols, catastrophes naturelles, etc.
- Assurance automobile: protège contre les dommages au véhicule pour collision, incendies, vols, vandalisme, etc.
- Assurance de biens commerciaux: protège le bâtiment, le contenu, etc.
:::

:::

::: {.column width='48%'}
[Responsabilité]{.two-blue-blocks}

<div style='height:70px;'></div>

<div style='padding-left:0px;'>
::: {.ul-font-size-medium}
- Assurance responsabilité civile: protège contre la responsabilité pour les blessures ou les dommages qu'ils peuvent causer à autrui
:::
</div>
:::

:::

::: notes
- En gros, l'assurance de dommages est tout ce qui n'est pas de l'assurance vie ni de l'assurance collective!
- Aussi appelée « assurance IARD », ou « property & casualty » en anglais.
- Le property, c'est pour les biens. Donc l'assurance de n'importe quel objet appartenant à l'assuré, que ce soit une voiture, une maison, etc.
- Le casualty, c'est la responsabilité. Ça protège contre la responsabilité pour les blessures ou les dommages causé à autrui.
- Par exemple, vous êtes obligé, lorsque vous conduisez une voiture au Québec, d'avoir une assurance responsabilité civile. Ça vous protège lorsque vous avez un accident responsable et que vous avez causé des blessures à quelqu'un ou des dommages à ses biens.
:::


## Assurance de dommage

### Qu'est-ce qu'on fait en assurance de dommage?

::: columns

::: {.column width='48%'}
[Rôles traditionnels]{.two-green-blocks}  

<div style='height:70px;'></div>

- Tarification
  - Indiqués
  - Segmentation
- Évaluation des réserves
  - IBNR (*Incurred But Not Reported*)
  - RBNS (*Reported But Not Settled*)
- Réassurance
:::

::: {.column width='48%'}
[Rôles non-traditionnels]{.two-green-blocks}

<div style='height:70px;'></div>

<div style='padding-left:0px;'>
- Intelligence d'affaires
- Analytique des réclamations
- Analytique de la souscription
</div>
:::

:::

::: notes
- Les rôles peuvent être séparés en rôles traditionnels et non-traditionnels.
- Les rôles traditionnels sont ceux qu'on associe plus naturellement au mot « actuariat ».
- Les rôles traditionnels sont le plus souvent occupés par des actuaires (parfois des Fellows), tandis que les rôles non-traditionnels peuvent être occupés par des actuaires, mais aussi par des scientifiques de donnnées, des programmeurs, etc.
- Dans les rôles traditionnels, on a la tarification, qui se sépare en 2 tâches. La première sont les indiqués (*rate indication*), où il faut essayer d'estimer le montant total des réclamations qu'on aura à payer l'an prochain. Ça sert à déterminer l'augmentation (ou, plus rarement, la diminution) des taux.
- La deuxième tâche est la segmentation, qui consiste à évaluer le risque associé à chaque groupe d'assurés pour fixer des primes d'assurance qui réflète leur niveau de risque. Cette étape est importante pour contrer l'anti-sélection.
- On a aussi l'évaluation des réserves. Ça consiste à estimer le montant à mettre de côté pour des réclamations qui sont arrivées mais qui n'ont pas été encore reportée à l'assureur, ou bien pour des réclamations qui sont reportées mais pas encore fermées.
- Ensuite, on a la réassurance. Par exemple, on doit s'assurer qu'on est capable de payer les réclamations en cas de catastrophe (par exemple, une catastrophe naturelle comme une inondation). Sinon, on devra peut-être revoir nos contrats de réassurance.

- Ensuite, il y a les rôles « non-traditionnels ».
- Ce sont des rôles où habituellement, la programmation est plus ominprésente. Chez Co-operators, on utilise beaucoup R et Python.
- Ce sont des rôles qui sont moins « actuariels », mais qui peuvent quand même être occupés par des actuaires, étant donné qu'ils ont une bonne expertise en statistiques, en programmation. Mais en plus, ça aide toujours d'avoir des connaissances en assurance quand on est dans une compangie d'assurance.
- Parmi ces rôles, on a l'intelligence d'affaires, l'analytique des réclamations et le underwriting analytics.
- Je dirais que l'analytique des réclamations et underwriting analytics, ça ressemble beaucoup à l'intelligence d'affaires, sauf que c'est plus axé sur optimiser le processus des réclamations et de l'underwriting (l'underwriting, c'est quand on vend une police d'assurance).
:::


## Co-operators

- Coopérative d'assurance de dommage fondée en 1945 par un groupe de fermiers 🧑‍🌾

**Notre mission**

- Assurer la sécurité financière des Canadiennes et Canadiens et de nos collectivités. Parce que nous sommes une coopérative, nos décisions d’affaires sont guidées par nos valeurs d’inclusion et de développement durable et responsable.


**Exemples d'engagements concrets**

- Investissements dans des fonds verts🌳💰
- Programme de séchage lors de dégâts d'eau spécifiques💧
- Assurance innondation🌊
- Programme « Build Back Better »🏗️

<!-- ![](images/auto.svg){.absolute width=200 top=450 left=0} -->
<!-- ![](images/maison.svg){.absolute width=200 top=450 left=200} -->
<!-- ![](images/entreprise.svg){.absolute width=200 top=450 left=600} -->
<!-- ![](images/ferme.svg){.absolute width=200 top=450 left=800} -->

<!-- [Lignes personnelles]{.absolute top=650 left=100} -->
<!-- [Lignes commerciales]{.absolute top=650 left=700} -->


::: notes
- Exemple de développement durable et responsable:
  - Investissement dans des fonds verts.
  - Programme de « Séchage » lors de dégâts d'eau spécifiques.
  - Assurance que les autres assureurs n'offrent pas: inondations.
  - Programme « Build Back Better »: dans certains cas comme les tempêtes de grêles, on s'engage à rebâtir avec des matériaux résistants à une prochaine tempête. On assume le cout additionel de reconstruction.
:::



## Qu'est-ce que l'intelligence d'affaires?

### Intelligence d'affaires = Tirer de la valeur des données

Les données ne parlent pas d'elles-mêmes: il faut les faire parler.

<br>

::: columns

::: {.column width='48%'}
[Avec...]{.two-blue-blocks}  

<div style='height:70px;'></div>

- Programmation R/Python (et autres)
- Machine learning/Deep learning
- Traitement de langage naturel
- Théorie des valeurs extrêmes
- Modèles de catastrophes
- Modèles statistiques classiques
- Etc.
:::

::: {.column width='48%'}
[...on produit]{.two-blue-blocks}

<div style='height:70px;'></div>

<div style='padding-left:0px;'>
- Modèles
- Tableaux de bord
- Applications Shiny (ou autre)
- Visualisations
- Analyses statistiques
- Etc.
</div>
:::

:::

![](images/dashboard.png){.absolute width=200 top=0 right=0}

::: notes
- 2 grands départements chez Co-operators: Actuariat et Intelligence d'Affaires. Qu'est-ce que l'intelligence d'affaires exactement?
- En fait, le but de l'intelligence d'affaire est de tirer le plus de valeur possible des données.
- Les données peuvent être sous n'importe quelle forme: données tabulaires, données textuelles, données temporelles, données d'images, données géospatiales, données audio, etc.
- Par contre, les données ne parlent pas d'elles-mêmes. Par exemple, on peut difficilement tirer des conclusions en observant à l'oeil une table de données. Il faut donc les faire « parler » en utilisant tous les moyens possibles: programmation, machine learning, deep learning, IA, théorie des valeurs extrêmes, et plusieurs autres modèles de toute sorte.
- On produit alors des outils qui nous aident à tirer profit des données et de prendre les bonnes décisions, comme des modèles, des applications, des tableaux de bord, des visualisations, etc.
:::


# 2. Exemple de projet

::: notes
- Maintenant, je vais vous donner des exemples de projets qu'on a faits dans le passé, avec un focus sur un projet que j'ai fait cette année et qui implique un modèle de langage, et donc qui permet de traiter du texte. 
- Tous les projets que je vais vous présenter ont été faits dans l'équipe de recherche et innovation analytique.
:::

## Sélecteur de codes IBC

#### Embeddings = Encoder une phrase (ou un mot) avec un vecteur de nombres réels

![](images/embeddings.jpg)

Les mots/phrases similaires doivent être proches dans l'espace d'embedding.

::: notes
- Donc je vous explique le problème qu'on avait à résoudre.
- Lorsqu'on assure une entreprise, on doit lui assigner un code appelé « code IBC ».
- Il y a 1260 de ces codes, donc ce n'est parfois pas évident de les assigner correctement.
- Ce sont des codes qui représente des secteurs de l'industrie. Par exemple: restaurant, ostéopathe, firme d'ingénieurs, etc.
- J'ai eu le mandat de développer un modèle qui permet de suggérer des codes étant donnée une description d'entreprise.
- Pour ce faire, j'ai utilisé un modèle d'embedding, c'est-à-dire un modèle qui permet de transformer du texte en nombres.

- Un embedding, c'est simplement une chaine de caractères encodée sous forme d'un vecteur.
- Ça peut être n'importe quelle chaine de caractères: un mot, un token, une phrase, un document, etc.
- Important parce qu'un ordinateur ne peut pas traiter du texte, seulement des nombres.
:::


## Sélecteur de codes IBC

### Espace d'embedding

- Un bon modèle d'embedding va positionner les mots (ou phrases) de sens similaire « près » les uns des autres dans l'espace d'embedding.
- Dans cet exemple, nous avons un embedding en 2 dimensions, mais en pratique, les embeddings possèdent généralement un nombre beaucoup plus élevé de dimensions.

![](images/embeddings_2.png)

::: notes
- Un bon modèle d'embeddings, c'est un modèle qui va placer des mots de sens similaire proches dans l'espace d'embeddings. À l'inverse, les mots de sens non-similaire doivent être éloignés.
- Ici par exemple, on a obtenu un embedding en 2D pour les mots « roi », « reine », « pomme » et « banane ».
- On voit que les embeddings créés semblent avoir du sens puisque « roi » et « reine », qui ont des sens similaires, sont proches dans l'espace d'embedding. Les embeddings de « pomme » et « banane » sont également proches, tandis que les 2 fruits et les 2 personnes sont éloignés.
:::


## Sélecteur de codes IBC

### « Interprétation » des dimensions

<br>
<br>
<br>
<br>

- Les valeurs des embeddings peuvent être imaginées étant des « concepts ».
- Par exemple, une des dimensions pourraient être le degré de « félinité ».
- En pratique, ces concepts sont obscures et pas humainement identifiables.

![](images/embeddings_as_concepts.png){.absolute top=100 right=0}

::: notes
- On peut avoir une pseudo-interprétation pour les dimensions des embeddings.
- Donc ici, on a des embeddings en 7 dimensions de plusieurs mots, et on peut s'imaginer que chaque dimension correspond à un concept.
- Donc peut-être que la dimension 1 correspond au degré de « vie », la dimension 2 correspond au degré de « félinité », la troisième au degré d'humanité, etc.
- Par exemple, le chat et le chaton ont un haut degré de félinité, tandis que la maison a un faible degré de félinité.
- En pratique par contre, on ne peut pas vraiment associer chaque dimension à un concept qu'on peut interpréter, comme ici avec le degré de « vivant » et de « félinité ». 
- Par contre, ça reste que chaque dimension est associée à un concept dont le modèle peut utiliser.
:::


## Sélecteur de codes IBC

### Résumé du problème

- Lorsqu’on assure une entreprise, on doit lui assigner un « code IBC » parmi 1260 codes. Étant donnée une description d’entreprise, on doit lui assigner le bon code

::: {.fragment .fade-in}

### Exemples de codes IBC

::: {style='font-size: 25px; position: absolute; top: 350px; left: 0px;'}
| Index | Code IBC | Description du code |
|-|--|----|
| 1 | 0735a10   | Architects - Landscaping Only |
| 2 | 8916a10 | Engineers |
| 3 | 8075a10 | Sanitariums |
| 4 | 5813a10 | Coffee shops (warming of food only, no roasting of beans) |
| ... | ... | ... |
| 1260 | 3496a10 | Electronic games |
:::

:::


## Sélecteur de codes IBC

<br>

Par exemple, quel code doit-on assigner à l'entreprise:
<br>
[Small business manufacturing and selling equipment for hockey referees]{style='color: #FFFFFF; background-color: #007d57; border-radius: 5px; padding: 2px 5px;'}?

<br>

**Comment procède-t-on pour assigner le bon code IBC?**

::: {.ol-font-size-medium}
1. On obtient un vecteur « d'embedding » représentant la description d'entreprise.
2. On obtient également un vecteur d'embedding pour chaque code IBC.
3. On calcule la distance entre l'entreprise et chaque code IBC.
4. Finalement, on choisit le code IBC qui est le plus proche de l'entreprise.
:::

Les embeddings sont obtenus avec un modèle de langage appelé « Sentence-BERT ».


![](images/sbert_2.png){.absolute width=170 top=160 right=30}


## Sélecteur de codes IBC

### BERT: fournit des embeddings de mots contextualisés de haute qualité

![](images/bert_puppet.png){.absolute top=164 right=30 width=350}

::: {width='100%'}
<br>

- BERT: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers
- Développé par **[G]{style='color:#174EA6;'}[o]{style='color:#A50E0E;'}[o]{style='color:#FBBC04;'}[g]{style='color:#174EA6;'}[l]{style='color:#0D652D;'}[e]{style='color:#A50E0E;'}** en 2018
:::

<br>

::: columns

::: {.column width='33%'}
[Pré-entrainé sur beaucoup de texte]{.three-green-blocks}  

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- BERT base: BooksCorpus (~800M mots)  
- BERT large: Wikipédia anglais (~2500M mots)
- Permet d'obtenir de bonnes performances même sans fine-tuning
:::
:::

::: {.column width='33%'}
[Embeddings contextualisés]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- Contexte bidirectionnel  
- D'anciens modèles comme Word2Vec et GloVe ne prenaient pas du tout en compte le contexte
:::
:::

::: {.column width='33%'}
[Open-source]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- On peut le télécharger gratuitement sur [HuggingFace.co](HuggingFace.co)
- On peut le fine-tuner sur une tâche particulière
:::
:::

:::

::: notes
- Le modèle qui sert de base à notre modèle Sentence-BERT d'embedding de phrases s'appelle BERT.
- C'est un modèle qui a été développé par Google et qui permet d'obtenir des embeddings de mots (ou plutôt de tokens).
- C'est un modèle qu'on dit « encoder-only », parce que contrairement à des modèles génératifs comme GPT, on utilise seulement la partie « encodeuse » du transformer.
- Donc le but n'est pas de générer du texte, mais seulement d'obtenir des embeddings de mots.

- C'est un modèle puissant pour 2 raisons principales.
- Premièrement, il a été pré-entrainé sur énormément de texte (sur plus de 3 milliards de mots), ce qui fait qu'il peut être utilisé directement sans être fine-tuné.
- Deuxièmement, c'est un modèle basé sur l'architecture transformer, qui permet d'obtenir des embeddings contextualisés de haute qualité, tout en étant hautement paraléllisable.
- C'est un modèle open-source, donc n'importe qui peut le télécharger et l'utiliser directement, ou bien le fine-tuner sur une tâche particulière.

- J'ajouterais également que BERT est relativement léger avec ses 110 millions de paramètres pour la version de base, ce qui permet de rapidement obtenir des embeddings, même sur un laptop.
:::




## Sélecteur de codes IBC

::: columns

::: {.column width='50%'}
**Pré-entrainement**  
![](images/pretraining.png){width='80%' fig-align='center'}
:::

::: {.column width='50%'}
**Fine-tuning**  
![](images/finetuning.png){width='78%' fig-align='center'}
:::

:::

::: notes
- Donc je vous ai parlé de pré-entrainement et de fine-tuning. À quoi ça correspond exactement?
- Le pré-entrainement, ça se résume à faire apprendre la langue au modèle. Donc les patterns de la langue, la grammaire, la syntaxe, etc.
- Pour ce faire, on va entrainer le modèle sur des grandes quantités de texte. Donc par exemple, sur Wikipédia au grand complet.
- Et pour BERT, la méthode utilisée pour le pré-entrainer s'appelle *masked language modeling*. Ça consiste à cacher certain mots au modèle de manière aléatoire, et on demande au modèle de deviner le mot. On a ensuite une fonction de perte qui va punir le modèle pour ses erreurs, et avec la rétro-propagation et la descente de gradient, on peut aller mettre à jour les poids du modèle.

- On peut utiliser le modèle pré-entrainé directement, mais on va avoir de meilleures performances si on le fine-tune sur notre tâche à accomplir.
- Donc dans l'étape du fine-tuning, on prend le modèel pré-entrainé et on l'entraine de nouveau sur notre jeu de données.
- Par exemple, ici on a fine-tuné BERT sur une tâche de classification de courriels (pourriel ou courriel légitime).
- On a ajouté une tête de classification au modèle, qui permet de transformer la sortie de BERT en probabilité d'être un pourriel.
- On va ensuite aller faire l'entrainement en utilisant un jeu de données de courriels étiquetés « spam » ou « not-spam ».
- Donc en gros, dans cette étape de fine-tuning, les poids du modèle vont être modifiés de telle sorte que la tâche à accomplir est optimisée.
:::


## Sélecteur de codes IBC

### Rappel du problème à résoudre

Lorsqu'on assure une entreprise, on doit lui assigner un code parmi 1260 codes. Étant donnée une description d'entreprise, on doit assigner le bon code.

::: {.fragment .fade-in}
### Données
:::

::: {.fragment .fade-in-then-out}

::: {style='font-size: 25px; position: absolute; top: 327px; left: 0px;'}
IBC Codes

| Index | IBC Code | IBC Code Description |
|-|--|----|
| 1 | 0735a10   | Architects - Landscaping Only |
| 2 | 8916a10 | Engineers |
| 3 | 8075a10 | Sanitariums |
| 4 | 5813a10 | Coffee shops (warming of food only, no roasting of beans) |
| ... | ... | ... |
| 1260 | 3496a10 | Electronic games |
:::

:::

::: {.fragment .fade-in}

::: {style='font-size: 25px; position: absolute; top: 327px; left: 0px;'}
Entreprises

| Index | Company Description | IBC Code |
|-|----|----|
| 1 | Takeout meals-see policy, coding only | 5814a10  -- Take out meals |
| 2 | Construction of church building extension | 1830a20  -- Building construction - general contr. |
| 3 | Construction and renovation of gas stations including installation and removal of underground tanks and related pumps, carwash facilities.  Snow clearing. | 1782a40 -- Underground storage tank - remediation contractors |
| ... | ... | ... |
| 136 000 | Licensed Restaurant | 5812a10  -- Restaurants - licensed (Not liquor revenue) |
:::

:::

::: notes
- La tâche qu'on veut accomplir est la suivante.
- Lorsqu'on assure une entreprise, le souscripteur doit lui assigner un code IBC parmi 1260 codes, ce qui n'est pas toujours évident.
- L'outil qu'on a créé permet de suggérer des codes IBC lorsqu'on fournit une description d'entreprise.
- Puisqu'on veut associer une entreprise à un code IBC, on va fine-tuner le modèle pour qu'il apprenne à rapprocher les entreprise de leur code IBC correspondant.

- Pour ça, on a 2 jeu de données. Le premier contient tous les codes et leur description, tandis que le deuxième contient 136 000 entreprises qu'on a assurées dans le passé avec leur code IBC associé.

:::

## Sélecteur de codes IBC

### Fine-tuner le modèle pré-entrainé

On génère d'abord 136 000 (autant qu'il y a d'entreprises) [**paires positives**]{style='color: #007d57;'} et 136 000 [**paires négatives**]{style='color: #650b22;'}.

::: {.ul-font-size-medium}
- [**paire positive**]{style='color: #007d57;'}: description de l'entreprise + description de son IBC code [**correspondant**]{style='color: #007d57;'}
- [**paire négative**]{style='color: #650b22;'}: description de l'entreprise + description d'un IBC code [**non-correspondant**]{style='color: #650b22;'}
:::


::: {style='font-size: 24px;'}
| Pair ID | Company Description | IBC Code Description | Label |
|-|----|----|---|
| 1 | Construction of church building extension | Building construction - general contr. | [**Positive pair**]{style='color: #007d57;'} |
| 2 | Construction of church building extension | Take out meals | [**Negative pair**]{style='color: #650b22;'} |
| 3 | Licensed restaurant | Restaurants - licensed (Not liquor revenue) | [**Positive pair**]{style='color: #007d57;'} |
| 4 | Licensed restaurant | Building construction - general contr. | [**Negative pair**]{style='color: #650b22;'} |
| ... | ... | ... | ... |
:::

On va ensuite entrainer le modèle à rapprocher les [**paires positives**]{style='color: #007d57;'} et à éloigner les [**paires négatives**]{style='color: #650b22;'} en utilisant une fonction de perte et la descente de gradient.


::: notes
- La première étape est de se créer un jeu de données étiqueté. On va se créer 136 00 paires de phrases positives et 136 000 paires négatives.
- Une paire positive, c'est une description d'entreprise couplée avec la description de son code IBC correspondant.
- Une paire négative, c'est une description d'entreprise couplée avec une description de code IBC non-correspondant.
- L'algorithme d'entrainement est alors déclenché pour modifier les paramètres du modèle de telle sorte que les entreprises sont rapprochées de leur IBC code correspondant (et éloignées de leurs codes IBC non-correspondants).
:::


## Sélecteur de codes IBC

### [Application Shiny](https://rstudiotest.cgic.ca/ibc_code_selector_app/)

<!-- ![](images/app_cap.png){.absolute width=350 top=170 left=0} -->
![](images/app_cap.png){width=420}


## Sélecteur de codes IBC  {visibility="uncounted"}

### Architecture de BERT: une pile d'encodeurs « transformer »

- BERT est un réseau de neurones composé de 12 ou 24 modules de type «\ transformer\ ».
- BERT « base »: 110 millions de paramètres.

[Transformer (ENCODER)]{.absolute top=650 left=50}

![](images/transformer-encoder.png){.absolute top=265 left=50 width=200}

![](images/bert-base-bert-large-encoders.png){.absolute top=240 left=500 width=700}

::: notes
- L'architecture de BERT ressemble à ça. En gros, c'est un pile de blocs « transformer », plus précisément la partie « encodeuse » seulement.
- La version de base contient 12 de ces blocs, tandis que la grande version en a 24.
- Donc un bloc transformer, ça ressemble à ça. En gros, ça reçoit un input et ça va le passer à travers plusieurs fonctions mathématiques, dont l'attention multi-tête, qui est la clé pour comprendre le contexte de mots (je vais parler du mécanisme d'attention dans la prochaine slide).
- Donc ce que le modèle fait en gros, c'est qu'il va partir d'un embedding fixe non-contextualisé (disons le embedding du mot « apple ») qu'il va passer à travers tous les blocs transformer pour obtenir un embedding de « apple » contextualisé.
- Et le rôle des blocs transformers en fait, c'est de modifier le embedding de « apple » en utilisant les embeddings de tous les mots qui sont autour du mot « apple ». 
:::


## Sélecteur de codes IBC  {visibility="uncounted"}

### Mécanisme d'attention: permet de contextualiser les embeddings

::: columns

::: {.column width='40%'}
![](images/attention.png){width="80%"}
:::

::: {.column width='60%'}
<br>
<br>
![](images/attention-luis.png){width="100%"}
:::

:::

::: notes
- Le mécanisme d'attention, ça a été développé en 2017 dans l'article « Attention is all you need », et c'est grâce à ça qu'on a aujourd'hui des grands modèles de langage comme BERT, GPT, Llama ou Gemini.
- En plus de bien capturer le contexte des mots, c'est une architecture qui est hautement parallélisable et qui permet donc d'entrainer rapidement les modèles si on a un GPU.
- Donc comme je vous disais, c'est le mécanisme d'attention dans le transformer qui permet de modifier les embeddings des mots en utilisant les embeddings des mots voisins, et ça fait ça en utilisant un mécanisme assez complexe.
- Donc supposons qu'on a un embedding non-contextualisé pour le mot « apple ». C'est un peu tannant, parce que « apple » peut désigner soit le fruit, soit la compagnie.
- Ce que l'attention fait, c'est qu'elle va faire bouger l'embedding de « apple » en utilisant les mots autour.
- Par exemple, dans la phrase « please buy me an apple and an orange », on a le mot « orange » qui va tirer le mot « apple » vers lui, ce qui va nous créer un embedding de « apple » spécialement fait pour cette phrase.
- De la même manière, dans la phrase « apple unveiled the new phone », l'embedding de « apple » va être tiré vers l'embedding du mot « phone », et donc dans cette phrase, « apple » a plus de chance d'être interprété comme étant la compagnie Apple.
- Il faut noter que « apple » (ainsi que tous les autres mots dans la fenêtre de contexte) va être influencé par tous les autres mots dans la fenêtre de contexte.
:::


## Sélecteur de codes IBC {visibility="uncounted"}

#### Sentence-BERT: modifier BERT pour encoder des phrases au lieu des mots

::: {.ul-font-size-medium}
- Architecture **siamoise** à l'entrainement utilisant un modèle **BERT pré-entrainé** (les poids sont identiques dans les 2 blocs BERT).
- La couche de **pooling** permet d'obtenir un  embedding de phrase à partir des embeddings de mots.
- Entrainé sur le corpus [SNLI](#id-SNLI): 570 000 paires de phrases annotées [**contradiction**]{style='color:#A50E0E;'}, [**implication**]{style='color:#0D652D;'} ou [**neutre**]{style='color: #a8aabc;'}.
:::

::: columns

::: {.column width='50%' style='text-align: center;'}
[À l'entrainement]{style='text-decoration: underline;'}

![](images/sbert_1.png){width=300 fig-align="center"}
:::

::: {.column width='50%' style='text-align: center;'}
[À l'inférence]{style='text-decoration: underline;'}

![](images/sbert_2.png){width=130 fig-align="center"}
:::

:::

[Figures tirées de [Hands-On Large Language Models -- Jay Alammar et Maarten Grootendorst](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)»]{.absolute bottom=-30 left='50%' style='font-size: 17px;'}

::: notes
- Donc ça nous amène à SBERT, qui est une modification de BERT pour faire des embeddings de phrases.
- C'est un réseau siamois (à l'entrainement) qui contient 2 modèles BERT avec les poids liés (en fait, c'est 2 copies du même modèle).
- On a une couche de pooling qui permet de transformer les embeddings de mots en embedding de phrase.
- Donc on prend BERT pré-entrainé et la couche de pooling qu'on arrange en un réseau siamois, et on va aller entrainer cette architecture sur un jeu de données de paires de phrases annotées contradiction, implication, ou neutre.
- Donc on prend chacune des 2 phrases, qu'on va passer au modèle BERT. Ensuite, la sortie de BERT va être donnée à la couche de pooling, qui va nous sortir un embedding de phrase. Ensuite, on va concaténer les 2 embeddings en plus de la différence absolue élément par élément, et c'est ce vecteur qui va être utilisé pour la classification entre « contradiction », « implication » ou « neutre ».

- Une fois le modèle entrainé, on peut se débarasser de l'architecture siamoise et garder seulement BERT + pooling.
- Donc en fait, ce qui améliore les embeddings de phrase, c'est vraiment l'étape d'entrainement sur les paires de phrases.
:::


## Sélecteur de codes IBC {visibility="uncounted"}

### Télécharger le modèle pré-entrainé

![](images/codeimage-snippet_21.svg){.absolute width=800 top=100 left=-60}

![](images/encode_snippet.svg){.absolute width=600 top=100 right=0}

::: {style='margin-top:310px; font-size: 25px;'}
| Phrase                      | dim_1     | dim_2     | ... | dim_768     |
|-----------------------------|-----------|-----------|-----|-------------|
| That is a happy person      | -0.0203 | -0.0104 | ... | -0.0053 |
| That is a happy dog         | -0.0196 | -0.0323 | ... | -0.0274 |
| That is a very happy person | -0.0136 | 0.0043  | ... | -0.0054 |
| Today is a sunny day        | -0.0075 | -0.0023 | ... | 0.0078  |
:::

On peut tester le modèle [ici](https://huggingface.co/sentence-transformers/all-mpnet-base-v2).

::: notes
- On peut facilement télécharger le modèle à partir du site HuggingFace en utilisant la librairie « sentence_transformers ».
- Le modèle qu'on utilise n'est pas exactement le SBERT original, mais quelque chose de très similaire. C'est le modèle de base, donc celui avec 12 blocs transformers et 110 millions de paramètres.
- On peut commencer à l'utiliser directement pour obtenir des embeddings de phrases avec la méthode « encode ».
:::


## Imagerie {visibility="uncounted"}

Estimer le coût de reconstruction des maisons.

![](images/imagerie.mp4){width=950}

[Précision de +/- 50k$]{style='margin-left:60px;'}

## Cartes d'inondations {visibility="uncounted"}

<br>

Améliorer les cartes d'inondation fournies par JBL, car celles-ci « pensaient » que les forêt sont des blocs qui ne laissent pas passer l'eau. 

::: colums

::: {.column width='49%'}
Avant

![](images/avant.png)
:::

::: {.column width='49%'}
Après

![](images/apres.png)
:::

:::


## Autres projets {visibility="uncounted"}

<br>

- Développer un modèle de tarification télématique pour le nouveau produit d'assurance télématique.
- Développer une application Shiny se basant sur le simulateur d'innondations pour mieux gérer notre risque de concentration.
- Améliorer le processus des indiqués en utilisant la théorie des valeurs extrêmes pour estimer une période de retour pour les grosses réclamations.
- « Fuzzy matching » pour l'équipe d'analytique de fraudes.


## Les défis de l'industrie {visibility="uncounted"}

<br>
**Dans aucun ordre particulier:**

- Fraude
- Catastrophes naturelles: inondations, séismes, feux de forêt, etc.
- Modélisation des grosses pertes
- Assurance télématique
- Traitement de langage naturel
- Etc.

![](images/icon_flood.png){.absolute width=400 bottom=0 right='10%'}
