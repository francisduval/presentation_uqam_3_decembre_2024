---
title: "L'intelligence d'affaires chez Co-operators"
lang: fr
subtitle: "PrÃ©sentation dans le cadre du cours Â« Applications probabilistes des risques actuariels Â»"
author:
  name: Francis Duval, Chercheur Scientifique | Recherche et Innovation Analytique
  email: francis_duval@cooperators.ca
date: 2025-10-07
date-format: long
format: 
  revealjs:
    template-partials:
      - title-slide.html
    theme: [default, styles.scss]
    width: 1280
    height: 720
    chalkboard: true
    transition: slide
    logo: images/logo_blue_2.png
    footer: '[https://github.com/francisduval/presentation_uqam_3_decembre_2024](https://github.com/francisduval/presentation_uqam_3_decembre_2024)'
    slide-number: c/t
    code-block-height: 500px
editor: 
  markdown: 
    wrap: 72
include-after-body: add-custom-footer.html
---

## Mon parcours

### AcadÃ©mique

::: columns

::: {.column width='33%'}
[BaccalaurÃ©at]{.three-green-blocks}  

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- **Actuariat**
- 2 stages chez Intact
- Session Ã  l'Ã©tranger Ã  Lyon
:::
:::

::: {.column width='33%'}
[Maitrise]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- **MathÃ©matiques actuarielles et financiÃ¨res**
- MÃ©moire: Techniques de gradient boosting pour la modÃ©lisation des rÃ©serves individuelles en assurance non-vie
- Stage de recherche Mitacs chez Desjardins
:::
:::

::: {.column width='33%'}
[Doctorat]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- **MathÃ©matiques**
- ThÃ¨se: ModÃ©lisation des sinistres en assurance automobile avec lâ€™utilisation de donnÃ©es tÃ©lÃ©matiques : approches dâ€™apprentissage automatique en classification et rÃ©gression de comptage
- Chaire Co-operators en analyse des risques actuariels
:::
:::

:::

### Professionel

- Chercheur scientifique dans l'Ã©quipe Â« Recherche et Innovation Analytique Â» depuis septembre 2023.

::: notes
- BaccalaurÃ©at en Actuariat, oÃ¹ j'ai eu 2 stages en assurance de dommages chez Intact.
- Session Ã  l'Ã©tranger Ã  l'Institut de Science FinanciÃ¨re et d'Assurance (ISFA).

- Ensuite, j'ai fait une maitrise en mathÃ©matiques actuarielles et financiÃ¨re, toujours Ã  l'UQAM.
- J'ai eu la chance durant ma maitrise de faire un stage de recherche Mitacs chez Desjardins. Mitacs est un organisme de recherche sans but lucratif qui offre des bourses de recherche, notamment pour des stages en entreprise.
- Durant ce stage chez Desjardins, j'ai dÃ©veloppÃ© un modÃ¨le d'Ã©valuation des rÃ©serves en utilisant un algorithme de gradient boosting. C'est un modÃ¨le de rÃ©serve au niveau individuel, c'est-Ã -dire que contrairement Ã  des modÃ¨les comme Chain-Ladder, on a essayer de prÃ©dire un montant pour chaque rÃ©clamation, de maniÃ¨re individuelle.
- Ce qui Ã©tait bien avec ce stage, c'est que le modÃ¨le que j'ai dÃ©veloppÃ© a servi pour mon mÃ©moire de maitrise, et j'ai mÃªme publiÃ© un article avec Mathieu Pigeon. Donc c'est faire d'une pierre deux coups.

- Suite Ã  ma maitrise, j'ai eu une offre de Jean-Philippe et Mathieu Pigeon pour continuer mes Ã©tudes au sein de la Chaire CARA, qui venait tout juste d'Ãªtre mise sur pieds. Comme j'avais aimÃ© mon expÃ©rience de recherche Ã  la maitrise, et que je trouvais que l'UQAM offrait une solide expertise en assurance de dommages avec des professeurs comme Jean-Philippe et Mathieu en plus du partenariat avec Co-operators, j'ai dÃ©cidÃ© de commencer un doctorat. De plus, le financement Ã©tait excellent.
- Dans le doctorat, il y a quelques cours au dÃ©but, mais le but est surtout de faire de la recherche. J'ai fait ce qu'on appelle une thÃ¨se par article, qui consiste en 3 articles. Les articles sont ensuite mis ensemble pour constituer la thÃ¨se.
- Ma thÃ¨se portait sur la modÃ©lisation des sinistres en assurance automobile avec des mÃ©thodes de machine learning utilisant des donnÃ©es tÃ©lÃ©matiques.

- Et depuis maintenant 2 ans, je suis chercheur scientifique dans l'Ã©quipe de Recherche et Innovation Analytique chez Co-operators, oÃ¹ j'ai dÃ©jÃ  eu l'occasion de travailler sur plusieurs projets.
:::


# 1. Introduction Ã  l'assurance de dommages, l'intelligence d'affaires, et Co-operators

## Assurance de dommage

- Aussi appelÃ©e Â« assurance IARD Â» (Incendie, Accidents et Risques Divers) c'est un type d'assurance qui protÃ¨ge les **individus** et les **entreprises** contre les pertes financiÃ¨res liÃ©es Ã  leurs **biens** et Ã  leur **responsabilitÃ©** pour les dommages qu'ils peuvent causer Ã  autrui.
- En anglais, **Property & Casualty (P&C) insurance** (property = Â« biens Â», casualty = Â« responsabilitÃ© Â»).

<div style='height:30px;'></div>

::: columns

::: {.column width='48%'}
[Biens]{.two-blue-blocks}  

<div style='height:70px;'></div>

::: {.ul-font-size-medium}
- Assurance habitation: protÃ¨ge contre les incendies, vols, catastrophes naturelles, etc.
- Assurance automobile: protÃ¨ge contre les dommages au vÃ©hicule pour collision, incendies, vols, vandalisme, etc.
- Assurance de biens commerciaux: protÃ¨ge le bÃ¢timent, le contenu, etc.
:::

:::

::: {.column width='48%'}
[ResponsabilitÃ©]{.two-blue-blocks}

<div style='height:70px;'></div>

<div style='padding-left:0px;'>
::: {.ul-font-size-medium}
- Assurance responsabilitÃ© civile: protÃ¨ge contre la responsabilitÃ© pour les blessures ou les dommages qu'ils peuvent causer Ã  autrui
:::
</div>
:::

:::

::: notes
- En gros, l'assurance de dommages est tout ce qui n'est pas de l'assurance vie ni de l'assurance collective!
- Aussi appelÃ©e Â« assurance IARD Â», ou Â« property & casualty Â» en anglais.
- Le property, c'est pour les biens. Donc l'assurance de n'importe quel objet appartenant Ã  l'assurÃ©, que ce soit une voiture, une maison, etc.
- Le casualty, c'est la responsabilitÃ©. Ã‡a protÃ¨ge contre la responsabilitÃ© pour les blessures ou les dommages causÃ© Ã  autrui.
- Par exemple, vous Ãªtes obligÃ©, lorsque vous conduisez une voiture au QuÃ©bec, d'avoir une assurance responsabilitÃ© civile. Ã‡a vous protÃ¨ge lorsque vous avez un accident responsable et que vous avez causÃ© des blessures Ã  quelqu'un ou des dommages Ã  ses biens.
:::


## Assurance de dommage

### Qu'est-ce qu'on fait en assurance de dommage?

::: columns

::: {.column width='48%'}
[RÃ´les traditionnels]{.two-green-blocks}  

<div style='height:70px;'></div>

- Tarification
  - IndiquÃ©s
  - Segmentation
- Ã‰valuation des rÃ©serves
  - IBNR (*Incurred But Not Reported*)
  - RBNS (*Reported But Not Settled*)
- RÃ©assurance
:::

::: {.column width='48%'}
[RÃ´les non-traditionnels]{.two-green-blocks}

<div style='height:70px;'></div>

<div style='padding-left:0px;'>
- Intelligence d'affaires
- Analytique des rÃ©clamations
- Analytique de la souscription
</div>
:::

:::

::: notes
- Les rÃ´les peuvent Ãªtre sÃ©parÃ©s en rÃ´les traditionnels et non-traditionnels.
- Les rÃ´les traditionnels sont ceux qu'on associe plus naturellement au mot Â« actuariat Â».
- Les rÃ´les traditionnels sont le plus souvent occupÃ©s par des actuaires (parfois des Fellows), tandis que les rÃ´les non-traditionnels peuvent Ãªtre occupÃ©s par des actuaires, mais aussi par des scientifiques de donnnÃ©es, des programmeurs, etc.
- Dans les rÃ´les traditionnels, on a la tarification, qui se sÃ©pare en 2 tÃ¢ches. La premiÃ¨re sont les indiquÃ©s (*rate indication*), oÃ¹ il faut essayer d'estimer le montant total des rÃ©clamations qu'on aura Ã  payer l'an prochain. Ã‡a sert Ã  dÃ©terminer l'augmentation (ou, plus rarement, la diminution) des taux.
- La deuxiÃ¨me tÃ¢che est la segmentation, qui consiste Ã  Ã©valuer le risque associÃ© Ã  chaque groupe d'assurÃ©s pour fixer des primes d'assurance qui rÃ©flÃ¨te leur niveau de risque. Cette Ã©tape est importante pour contrer l'anti-sÃ©lection.
- On a aussi l'Ã©valuation des rÃ©serves. Ã‡a consiste Ã  estimer le montant Ã  mettre de cÃ´tÃ© pour des rÃ©clamations qui sont arrivÃ©es mais qui n'ont pas Ã©tÃ© encore reportÃ©e Ã  l'assureur, ou bien pour des rÃ©clamations qui sont reportÃ©es mais pas encore fermÃ©es.
- Ensuite, on a la rÃ©assurance. Par exemple, on doit s'assurer qu'on est capable de payer les rÃ©clamations en cas de catastrophe (par exemple, une catastrophe naturelle comme une inondation). Sinon, on devra peut-Ãªtre revoir nos contrats de rÃ©assurance.

- Ensuite, il y a les rÃ´les Â« non-traditionnels Â».
- Ce sont des rÃ´les oÃ¹ habituellement, la programmation est plus ominprÃ©sente. Chez Co-operators, on utilise beaucoup R et Python.
- Ce sont des rÃ´les qui sont moins Â« actuariels Â», mais qui peuvent quand mÃªme Ãªtre occupÃ©s par des actuaires, Ã©tant donnÃ© qu'ils ont une bonne expertise en statistiques, en programmation. Mais en plus, Ã§a aide toujours d'avoir des connaissances en assurance quand on est dans une compangie d'assurance.
- Parmi ces rÃ´les, on a l'intelligence d'affaires, l'analytique des rÃ©clamations et le underwriting analytics.
- Je dirais que l'analytique des rÃ©clamations et underwriting analytics, Ã§a ressemble beaucoup Ã  l'intelligence d'affaires, sauf que c'est plus axÃ© sur optimiser le processus des rÃ©clamations et de l'underwriting (l'underwriting, c'est quand on vend une police d'assurance).
:::


## Co-operators

- CoopÃ©rative d'assurance de dommage fondÃ©e en 1945 par un groupe de fermiers ğŸ§‘â€ğŸŒ¾

**Notre mission**

- Assurer la sÃ©curitÃ© financiÃ¨re des Canadiennes et Canadiens et de nos collectivitÃ©s. Parce que nous sommes une coopÃ©rative, nos dÃ©cisions dâ€™affaires sont guidÃ©es par nos valeurs dâ€™inclusion et de dÃ©veloppement durable et responsable.


**Exemples d'engagements concrets**

- Investissements dans des fonds vertsğŸŒ³ğŸ’°
- Programme de sÃ©chage lors de dÃ©gÃ¢ts d'eau spÃ©cifiquesğŸ’§
- Assurance innondationğŸŒŠ
- Programme Â« Build Back Better Â»ğŸ—ï¸

<!-- ![](images/auto.svg){.absolute width=200 top=450 left=0} -->
<!-- ![](images/maison.svg){.absolute width=200 top=450 left=200} -->
<!-- ![](images/entreprise.svg){.absolute width=200 top=450 left=600} -->
<!-- ![](images/ferme.svg){.absolute width=200 top=450 left=800} -->

<!-- [Lignes personnelles]{.absolute top=650 left=100} -->
<!-- [Lignes commerciales]{.absolute top=650 left=700} -->


::: notes
- Exemple de dÃ©veloppement durable et responsable:
  - Investissement dans des fonds verts.
  - Programme de Â« SÃ©chage Â» lors de dÃ©gÃ¢ts d'eau spÃ©cifiques.
  - Assurance que les autres assureurs n'offrent pas: inondations.
  - Programme Â« Build Back Better Â»: dans certains cas comme les tempÃªtes de grÃªles, on s'engage Ã  rebÃ¢tir avec des matÃ©riaux rÃ©sistants Ã  une prochaine tempÃªte. On assume le cout additionel de reconstruction.
:::



## Qu'est-ce que l'intelligence d'affaires?

### Intelligence d'affaires = Tirer de la valeur des donnÃ©es

Les donnÃ©es ne parlent pas d'elles-mÃªmes: il faut les faire parler.

<br>

::: columns

::: {.column width='48%'}
[Avec...]{.two-blue-blocks}  

<div style='height:70px;'></div>

- Programmation R/Python (et autres)
- Machine learning/Deep learning
- Traitement de langage naturel
- ThÃ©orie des valeurs extrÃªmes
- ModÃ¨les de catastrophes
- ModÃ¨les statistiques classiques
- Etc.
:::

::: {.column width='48%'}
[...on produit]{.two-blue-blocks}

<div style='height:70px;'></div>

<div style='padding-left:0px;'>
- ModÃ¨les
- Tableaux de bord
- Applications Shiny (ou autre)
- Visualisations
- Analyses statistiques
- Etc.
</div>
:::

:::

![](images/dashboard.png){.absolute width=200 top=0 right=0}

::: notes
- 2 grands dÃ©partements chez Co-operators: Actuariat et Intelligence d'Affaires. Qu'est-ce que l'intelligence d'affaires exactement?
- En fait, le but de l'intelligence d'affaire est de tirer le plus de valeur possible des donnÃ©es.
- Les donnÃ©es peuvent Ãªtre sous n'importe quelle forme: donnÃ©es tabulaires, donnÃ©es textuelles, donnÃ©es temporelles, donnÃ©es d'images, donnÃ©es gÃ©ospatiales, donnÃ©es audio, etc.
- Par contre, les donnÃ©es ne parlent pas d'elles-mÃªmes. Par exemple, on peut difficilement tirer des conclusions en observant Ã  l'oeil une table de donnÃ©es. Il faut donc les faire Â« parler Â» en utilisant tous les moyens possibles: programmation, machine learning, deep learning, IA, thÃ©orie des valeurs extrÃªmes, et plusieurs autres modÃ¨les de toute sorte.
- On produit alors des outils qui nous aident Ã  tirer profit des donnÃ©es et de prendre les bonnes dÃ©cisions, comme des modÃ¨les, des applications, des tableaux de bord, des visualisations, etc.
:::


# 2. Exemple de projet

::: notes
- Maintenant, je vais vous donner des exemples de projets qu'on a faits dans le passÃ©, avec un focus sur un projet que j'ai fait cette annÃ©e et qui implique un modÃ¨le de langage, et donc qui permet de traiter du texte. 
- Tous les projets que je vais vous prÃ©senter ont Ã©tÃ© faits dans l'Ã©quipe de recherche et innovation analytique.
:::

## SÃ©lecteur de codes IBC

#### Embeddings = Encoder une phrase (ou un mot) avec un vecteur de nombres rÃ©els

![](images/embeddings.jpg)

Les mots/phrases similaires doivent Ãªtre proches dans l'espace d'embedding.

::: notes
- Donc je vous explique le problÃ¨me qu'on avait Ã  rÃ©soudre.
- Lorsqu'on assure une entreprise, on doit lui assigner un code appelÃ© Â« code IBC Â».
- Il y a 1260 de ces codes, donc ce n'est parfois pas Ã©vident de les assigner correctement.
- Ce sont des codes qui reprÃ©sente des secteurs de l'industrie. Par exemple: restaurant, ostÃ©opathe, firme d'ingÃ©nieurs, etc.
- J'ai eu le mandat de dÃ©velopper un modÃ¨le qui permet de suggÃ©rer des codes Ã©tant donnÃ©e une description d'entreprise.
- Pour ce faire, j'ai utilisÃ© un modÃ¨le d'embedding, c'est-Ã -dire un modÃ¨le qui permet de transformer du texte en nombres.

- Un embedding, c'est simplement une chaine de caractÃ¨res encodÃ©e sous forme d'un vecteur.
- Ã‡a peut Ãªtre n'importe quelle chaine de caractÃ¨res: un mot, un token, une phrase, un document, etc.
- Important parce qu'un ordinateur ne peut pas traiter du texte, seulement des nombres.
:::


## SÃ©lecteur de codes IBC

### Espace d'embedding

- Un bon modÃ¨le d'embedding va positionner les mots (ou phrases) de sens similaire Â« prÃ¨s Â» les uns des autres dans l'espace d'embedding.
- Dans cet exemple, nous avons un embedding en 2 dimensions, mais en pratique, les embeddings possÃ¨dent gÃ©nÃ©ralement un nombre beaucoup plus Ã©levÃ© de dimensions.

![](images/embeddings_2.png)

::: notes
- Un bon modÃ¨le d'embeddings, c'est un modÃ¨le qui va placer des mots de sens similaire proches dans l'espace d'embeddings. Ã€ l'inverse, les mots de sens non-similaire doivent Ãªtre Ã©loignÃ©s.
- Ici par exemple, on a obtenu un embedding en 2D pour les mots Â« roi Â», Â« reine Â», Â« pomme Â» et Â« banane Â».
- On voit que les embeddings crÃ©Ã©s semblent avoir du sens puisque Â« roi Â» et Â« reine Â», qui ont des sens similaires, sont proches dans l'espace d'embedding. Les embeddings de Â« pomme Â» et Â« banane Â» sont Ã©galement proches, tandis que les 2 fruits et les 2 personnes sont Ã©loignÃ©s.
:::


## SÃ©lecteur de codes IBC

### Â« InterprÃ©tation Â» des dimensions

<br>
<br>
<br>
<br>

- Les valeurs des embeddings peuvent Ãªtre imaginÃ©es Ã©tant des Â« concepts Â».
- Par exemple, une des dimensions pourraient Ãªtre le degrÃ© de Â« fÃ©linitÃ© Â».
- En pratique, ces concepts sont obscures et pas humainement identifiables.

![](images/embeddings_as_concepts.png){.absolute top=100 right=0}

::: notes
- On peut avoir une pseudo-interprÃ©tation pour les dimensions des embeddings.
- Donc ici, on a des embeddings en 7 dimensions de plusieurs mots, et on peut s'imaginer que chaque dimension correspond Ã  un concept.
- Donc peut-Ãªtre que la dimension 1 correspond au degrÃ© de Â« vie Â», la dimension 2 correspond au degrÃ© de Â« fÃ©linitÃ© Â», la troisiÃ¨me au degrÃ© d'humanitÃ©, etc.
- Par exemple, le chat et le chaton ont un haut degrÃ© de fÃ©linitÃ©, tandis que la maison a un faible degrÃ© de fÃ©linitÃ©.
- En pratique par contre, on ne peut pas vraiment associer chaque dimension Ã  un concept qu'on peut interprÃ©ter, comme ici avec le degrÃ© de Â« vivant Â» et de Â« fÃ©linitÃ© Â». 
- Par contre, Ã§a reste que chaque dimension est associÃ©e Ã  un concept dont le modÃ¨le peut utiliser.
:::


## SÃ©lecteur de codes IBC

### RÃ©sumÃ© du problÃ¨me

- Lorsquâ€™on assure une entreprise, on doit lui assigner un Â« code IBC Â» parmi 1260 codes. Ã‰tant donnÃ©e une description dâ€™entreprise, on doit lui assigner le bon code

::: {.fragment .fade-in}

### Exemples de codes IBC

::: {style='font-size: 25px; position: absolute; top: 350px; left: 0px;'}
| Index | Code IBC | Description du code |
|-|--|----|
| 1 | 0735a10   | Architects - Landscaping Only |
| 2 | 8916a10 | Engineers |
| 3 | 8075a10 | Sanitariums |
| 4 | 5813a10 | Coffee shops (warming of food only, no roasting of beans) |
| ... | ... | ... |
| 1260 | 3496a10 | Electronic games |
:::

:::


## SÃ©lecteur de codes IBC

<br>

Par exemple, quel code doit-on assigner Ã  l'entreprise:
<br>
[Small business manufacturing and selling equipment for hockey referees]{style='color: #FFFFFF; background-color: #007d57; border-radius: 5px; padding: 2px 5px;'}?

<br>

**Comment procÃ¨de-t-on pour assigner le bon code IBC?**

::: {.ol-font-size-medium}
1. On obtient un vecteur Â« d'embedding Â» reprÃ©sentant la description d'entreprise.
2. On obtient Ã©galement un vecteur d'embedding pour chaque code IBC.
3. On calcule la distance entre l'entreprise et chaque code IBC.
4. Finalement, on choisit le code IBC qui est le plus proche de l'entreprise.
:::

Les embeddings sont obtenus avec un modÃ¨le de langage appelÃ© Â« Sentence-BERT Â».


![](images/sbert_2.png){.absolute width=170 top=160 right=30}


## SÃ©lecteur de codes IBC

### BERT: fournit des embeddings de mots contextualisÃ©s de haute qualitÃ©

![](images/bert_puppet.png){.absolute top=164 right=30 width=350}

::: {width='100%'}
<br>

- BERT: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers
- DÃ©veloppÃ© par **[G]{style='color:#174EA6;'}[o]{style='color:#A50E0E;'}[o]{style='color:#FBBC04;'}[g]{style='color:#174EA6;'}[l]{style='color:#0D652D;'}[e]{style='color:#A50E0E;'}** en 2018
:::

<br>

::: columns

::: {.column width='33%'}
[PrÃ©-entrainÃ© sur beaucoup de texte]{.three-green-blocks}  

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- BERT base: BooksCorpus (~800M mots)  
- BERT large: WikipÃ©dia anglais (~2500M mots)
- Permet d'obtenir de bonnes performances mÃªme sans fine-tuning
:::
:::

::: {.column width='33%'}
[Embeddings contextualisÃ©s]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- Contexte bidirectionnel  
- D'anciens modÃ¨les comme Word2Vec et GloVe ne prenaient pas du tout en compte le contexte
:::
:::

::: {.column width='33%'}
[Open-source]{.three-green-blocks}

<div style='height:70px;'></div>

::: {.ul-font-size-small}
- On peut le tÃ©lÃ©charger gratuitement sur [HuggingFace.co](HuggingFace.co)
- On peut le fine-tuner sur une tÃ¢che particuliÃ¨re
:::
:::

:::

::: notes
- Le modÃ¨le qui sert de base Ã  notre modÃ¨le Sentence-BERT d'embedding de phrases s'appelle BERT.
- C'est un modÃ¨le qui a Ã©tÃ© dÃ©veloppÃ© par Google et qui permet d'obtenir des embeddings de mots (ou plutÃ´t de tokens).
- C'est un modÃ¨le qu'on dit Â« encoder-only Â», parce que contrairement Ã  des modÃ¨les gÃ©nÃ©ratifs comme GPT, on utilise seulement la partie Â« encodeuse Â» du transformer.
- Donc le but n'est pas de gÃ©nÃ©rer du texte, mais seulement d'obtenir des embeddings de mots.

- C'est un modÃ¨le puissant pour 2 raisons principales.
- PremiÃ¨rement, il a Ã©tÃ© prÃ©-entrainÃ© sur Ã©normÃ©ment de texte (sur plus de 3 milliards de mots), ce qui fait qu'il peut Ãªtre utilisÃ© directement sans Ãªtre fine-tunÃ©.
- DeuxiÃ¨mement, c'est un modÃ¨le basÃ© sur l'architecture transformer, qui permet d'obtenir des embeddings contextualisÃ©s de haute qualitÃ©, tout en Ã©tant hautement paralÃ©llisable.
- C'est un modÃ¨le open-source, donc n'importe qui peut le tÃ©lÃ©charger et l'utiliser directement, ou bien le fine-tuner sur une tÃ¢che particuliÃ¨re.

- J'ajouterais Ã©galement que BERT est relativement lÃ©ger avec ses 110 millions de paramÃ¨tres pour la version de base, ce qui permet de rapidement obtenir des embeddings, mÃªme sur un laptop.
:::




## SÃ©lecteur de codes IBC

::: columns

::: {.column width='50%'}
**PrÃ©-entrainement**  
![](images/pretraining.png){width='80%' fig-align='center'}
:::

::: {.column width='50%'}
**Fine-tuning**  
![](images/finetuning.png){width='78%' fig-align='center'}
:::

:::

::: notes
- Donc je vous ai parlÃ© de prÃ©-entrainement et de fine-tuning. Ã€ quoi Ã§a correspond exactement?
- Le prÃ©-entrainement, Ã§a se rÃ©sume Ã  faire apprendre la langue au modÃ¨le. Donc les patterns de la langue, la grammaire, la syntaxe, etc.
- Pour ce faire, on va entrainer le modÃ¨le sur des grandes quantitÃ©s de texte. Donc par exemple, sur WikipÃ©dia au grand complet.
- Et pour BERT, la mÃ©thode utilisÃ©e pour le prÃ©-entrainer s'appelle *masked language modeling*. Ã‡a consiste Ã  cacher certain mots au modÃ¨le de maniÃ¨re alÃ©atoire, et on demande au modÃ¨le de deviner le mot. On a ensuite une fonction de perte qui va punir le modÃ¨le pour ses erreurs, et avec la rÃ©tro-propagation et la descente de gradient, on peut aller mettre Ã  jour les poids du modÃ¨le.

- On peut utiliser le modÃ¨le prÃ©-entrainÃ© directement, mais on va avoir de meilleures performances si on le fine-tune sur notre tÃ¢che Ã  accomplir.
- Donc dans l'Ã©tape du fine-tuning, on prend le modÃ¨el prÃ©-entrainÃ© et on l'entraine de nouveau sur notre jeu de donnÃ©es.
- Par exemple, ici on a fine-tunÃ© BERT sur une tÃ¢che de classification de courriels (pourriel ou courriel lÃ©gitime).
- On a ajoutÃ© une tÃªte de classification au modÃ¨le, qui permet de transformer la sortie de BERT en probabilitÃ© d'Ãªtre un pourriel.
- On va ensuite aller faire l'entrainement en utilisant un jeu de donnÃ©es de courriels Ã©tiquetÃ©s Â« spam Â» ou Â« not-spam Â».
- Donc en gros, dans cette Ã©tape de fine-tuning, les poids du modÃ¨le vont Ãªtre modifiÃ©s de telle sorte que la tÃ¢che Ã  accomplir est optimisÃ©e.
:::


## SÃ©lecteur de codes IBC

### Rappel du problÃ¨me Ã  rÃ©soudre

Lorsqu'on assure une entreprise, on doit lui assigner un code parmi 1260 codes. Ã‰tant donnÃ©e une description d'entreprise, on doit assigner le bon code.

::: {.fragment .fade-in}
### DonnÃ©es
:::

::: {.fragment .fade-in-then-out}

::: {style='font-size: 25px; position: absolute; top: 327px; left: 0px;'}
IBC Codes

| Index | IBC Code | IBC Code Description |
|-|--|----|
| 1 | 0735a10   | Architects - Landscaping Only |
| 2 | 8916a10 | Engineers |
| 3 | 8075a10 | Sanitariums |
| 4 | 5813a10 | Coffee shops (warming of food only, no roasting of beans) |
| ... | ... | ... |
| 1260 | 3496a10 | Electronic games |
:::

:::

::: {.fragment .fade-in}

::: {style='font-size: 25px; position: absolute; top: 327px; left: 0px;'}
Entreprises

| Index | Company Description | IBC Code |
|-|----|----|
| 1 | Takeout meals-see policy, coding only | 5814a10  -- Take out meals |
| 2 | Construction of church building extension | 1830a20  -- Building construction - general contr. |
| 3 | Construction and renovation of gas stations including installation and removal of underground tanks and related pumps, carwash facilities.  Snow clearing. | 1782a40 -- Underground storage tank - remediation contractors |
| ... | ... | ... |
| 136 000 | Licensed Restaurant | 5812a10  -- Restaurants - licensed (Not liquor revenue) |
:::

:::

::: notes
- La tÃ¢che qu'on veut accomplir est la suivante.
- Lorsqu'on assure une entreprise, le souscripteur doit lui assigner un code IBC parmi 1260 codes, ce qui n'est pas toujours Ã©vident.
- L'outil qu'on a crÃ©Ã© permet de suggÃ©rer des codes IBC lorsqu'on fournit une description d'entreprise.
- Puisqu'on veut associer une entreprise Ã  un code IBC, on va fine-tuner le modÃ¨le pour qu'il apprenne Ã  rapprocher les entreprise de leur code IBC correspondant.

- Pour Ã§a, on a 2 jeu de donnÃ©es. Le premier contient tous les codes et leur description, tandis que le deuxiÃ¨me contient 136 000 entreprises qu'on a assurÃ©es dans le passÃ© avec leur code IBC associÃ©.

:::

## SÃ©lecteur de codes IBC

### Fine-tuner le modÃ¨le prÃ©-entrainÃ©

On gÃ©nÃ¨re d'abord 136 000 (autant qu'il y a d'entreprises) [**paires positives**]{style='color: #007d57;'} et 136 000 [**paires nÃ©gatives**]{style='color: #650b22;'}.

::: {.ul-font-size-medium}
- [**paire positive**]{style='color: #007d57;'}: description de l'entreprise + description de son IBC code [**correspondant**]{style='color: #007d57;'}
- [**paire nÃ©gative**]{style='color: #650b22;'}: description de l'entreprise + description d'un IBC code [**non-correspondant**]{style='color: #650b22;'}
:::


::: {style='font-size: 24px;'}
| Pair ID | Company Description | IBC Code Description | Label |
|-|----|----|---|
| 1 | Construction of church building extension | Building construction - general contr. | [**Positive pair**]{style='color: #007d57;'} |
| 2 | Construction of church building extension | Take out meals | [**Negative pair**]{style='color: #650b22;'} |
| 3 | Licensed restaurant | Restaurants - licensed (Not liquor revenue) | [**Positive pair**]{style='color: #007d57;'} |
| 4 | Licensed restaurant | Building construction - general contr. | [**Negative pair**]{style='color: #650b22;'} |
| ... | ... | ... | ... |
:::

On va ensuite entrainer le modÃ¨le Ã  rapprocher les [**paires positives**]{style='color: #007d57;'} et Ã  Ã©loigner les [**paires nÃ©gatives**]{style='color: #650b22;'} en utilisant une fonction de perte et la descente de gradient.


::: notes
- La premiÃ¨re Ã©tape est de se crÃ©er un jeu de donnÃ©es Ã©tiquetÃ©. On va se crÃ©er 136 00 paires de phrases positives et 136 000 paires nÃ©gatives.
- Une paire positive, c'est une description d'entreprise couplÃ©e avec la description de son code IBC correspondant.
- Une paire nÃ©gative, c'est une description d'entreprise couplÃ©e avec une description de code IBC non-correspondant.
- L'algorithme d'entrainement est alors dÃ©clenchÃ© pour modifier les paramÃ¨tres du modÃ¨le de telle sorte que les entreprises sont rapprochÃ©es de leur IBC code correspondant (et Ã©loignÃ©es de leurs codes IBC non-correspondants).
:::


## SÃ©lecteur de codes IBC

### [Application Shiny](https://rstudiotest.cgic.ca/ibc_code_selector_app/)

<!-- ![](images/app_cap.png){.absolute width=350 top=170 left=0} -->
![](images/app_cap.png){width=420}


## SÃ©lecteur de codes IBC  {visibility="uncounted"}

### Architecture de BERT: une pile d'encodeurs Â« transformer Â»

- BERT est un rÃ©seau de neurones composÃ© de 12 ou 24 modules de type Â«\ transformer\ Â».
- BERT Â« base Â»: 110 millions de paramÃ¨tres.

[Transformer (ENCODER)]{.absolute top=650 left=50}

![](images/transformer-encoder.png){.absolute top=265 left=50 width=200}

![](images/bert-base-bert-large-encoders.png){.absolute top=240 left=500 width=700}

::: notes
- L'architecture de BERT ressemble Ã  Ã§a. En gros, c'est un pile de blocs Â« transformer Â», plus prÃ©cisÃ©ment la partie Â« encodeuse Â» seulement.
- La version de base contient 12 de ces blocs, tandis que la grande version en a 24.
- Donc un bloc transformer, Ã§a ressemble Ã  Ã§a. En gros, Ã§a reÃ§oit un input et Ã§a va le passer Ã  travers plusieurs fonctions mathÃ©matiques, dont l'attention multi-tÃªte, qui est la clÃ© pour comprendre le contexte de mots (je vais parler du mÃ©canisme d'attention dans la prochaine slide).
- Donc ce que le modÃ¨le fait en gros, c'est qu'il va partir d'un embedding fixe non-contextualisÃ© (disons le embedding du mot Â« apple Â») qu'il va passer Ã  travers tous les blocs transformer pour obtenir un embedding de Â« apple Â» contextualisÃ©.
- Et le rÃ´le des blocs transformers en fait, c'est de modifier le embedding de Â« apple Â» en utilisant les embeddings de tous les mots qui sont autour du mot Â« apple Â». 
:::


## SÃ©lecteur de codes IBC  {visibility="uncounted"}

### MÃ©canisme d'attention: permet de contextualiser les embeddings

::: columns

::: {.column width='40%'}
![](images/attention.png){width="80%"}
:::

::: {.column width='60%'}
<br>
<br>
![](images/attention-luis.png){width="100%"}
:::

:::

::: notes
- Le mÃ©canisme d'attention, Ã§a a Ã©tÃ© dÃ©veloppÃ© en 2017 dans l'article Â« Attention is all you need Â», et c'est grÃ¢ce Ã  Ã§a qu'on a aujourd'hui des grands modÃ¨les de langage comme BERT, GPT, Llama ou Gemini.
- En plus de bien capturer le contexte des mots, c'est une architecture qui est hautement parallÃ©lisable et qui permet donc d'entrainer rapidement les modÃ¨les si on a un GPU.
- Donc comme je vous disais, c'est le mÃ©canisme d'attention dans le transformer qui permet de modifier les embeddings des mots en utilisant les embeddings des mots voisins, et Ã§a fait Ã§a en utilisant un mÃ©canisme assez complexe.
- Donc supposons qu'on a un embedding non-contextualisÃ© pour le mot Â« apple Â». C'est un peu tannant, parce que Â« apple Â» peut dÃ©signer soit le fruit, soit la compagnie.
- Ce que l'attention fait, c'est qu'elle va faire bouger l'embedding de Â« apple Â» en utilisant les mots autour.
- Par exemple, dans la phrase Â« please buy me an apple and an orange Â», on a le mot Â« orange Â» qui va tirer le mot Â« apple Â» vers lui, ce qui va nous crÃ©er un embedding de Â« apple Â» spÃ©cialement fait pour cette phrase.
- De la mÃªme maniÃ¨re, dans la phrase Â« apple unveiled the new phone Â», l'embedding de Â« apple Â» va Ãªtre tirÃ© vers l'embedding du mot Â« phone Â», et donc dans cette phrase, Â« apple Â» a plus de chance d'Ãªtre interprÃ©tÃ© comme Ã©tant la compagnie Apple.
- Il faut noter que Â« apple Â» (ainsi que tous les autres mots dans la fenÃªtre de contexte) va Ãªtre influencÃ© par tous les autres mots dans la fenÃªtre de contexte.
:::


## SÃ©lecteur de codes IBC {visibility="uncounted"}

#### Sentence-BERT: modifier BERT pour encoder des phrases au lieu des mots

::: {.ul-font-size-medium}
- Architecture **siamoise** Ã  l'entrainement utilisant un modÃ¨le **BERT prÃ©-entrainÃ©** (les poids sont identiques dans les 2 blocs BERT).
- La couche de **pooling** permet d'obtenir un  embedding de phrase Ã  partir des embeddings de mots.
- EntrainÃ© sur le corpus [SNLI](#id-SNLI): 570 000 paires de phrases annotÃ©es [**contradiction**]{style='color:#A50E0E;'}, [**implication**]{style='color:#0D652D;'} ou [**neutre**]{style='color: #a8aabc;'}.
:::

::: columns

::: {.column width='50%' style='text-align: center;'}
[Ã€ l'entrainement]{style='text-decoration: underline;'}

![](images/sbert_1.png){width=300 fig-align="center"}
:::

::: {.column width='50%' style='text-align: center;'}
[Ã€ l'infÃ©rence]{style='text-decoration: underline;'}

![](images/sbert_2.png){width=130 fig-align="center"}
:::

:::

[Figures tirÃ©es de [Hands-On Large Language Models -- Jay Alammar et Maarten Grootendorst](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)Â»]{.absolute bottom=-30 left='50%' style='font-size: 17px;'}

::: notes
- Donc Ã§a nous amÃ¨ne Ã  SBERT, qui est une modification de BERT pour faire des embeddings de phrases.
- C'est un rÃ©seau siamois (Ã  l'entrainement) qui contient 2 modÃ¨les BERT avec les poids liÃ©s (en fait, c'est 2 copies du mÃªme modÃ¨le).
- On a une couche de pooling qui permet de transformer les embeddings de mots en embedding de phrase.
- Donc on prend BERT prÃ©-entrainÃ© et la couche de pooling qu'on arrange en un rÃ©seau siamois, et on va aller entrainer cette architecture sur un jeu de donnÃ©es de paires de phrases annotÃ©es contradiction, implication, ou neutre.
- Donc on prend chacune des 2 phrases, qu'on va passer au modÃ¨le BERT. Ensuite, la sortie de BERT va Ãªtre donnÃ©e Ã  la couche de pooling, qui va nous sortir un embedding de phrase. Ensuite, on va concatÃ©ner les 2 embeddings en plus de la diffÃ©rence absolue Ã©lÃ©ment par Ã©lÃ©ment, et c'est ce vecteur qui va Ãªtre utilisÃ© pour la classification entre Â« contradiction Â», Â« implication Â» ou Â« neutre Â».

- Une fois le modÃ¨le entrainÃ©, on peut se dÃ©barasser de l'architecture siamoise et garder seulement BERT + pooling.
- Donc en fait, ce qui amÃ©liore les embeddings de phrase, c'est vraiment l'Ã©tape d'entrainement sur les paires de phrases.
:::


## SÃ©lecteur de codes IBC {visibility="uncounted"}

### TÃ©lÃ©charger le modÃ¨le prÃ©-entrainÃ©

![](images/codeimage-snippet_21.svg){.absolute width=800 top=100 left=-60}

![](images/encode_snippet.svg){.absolute width=600 top=100 right=0}

::: {style='margin-top:310px; font-size: 25px;'}
| Phrase                      | dim_1     | dim_2     | ... | dim_768     |
|-----------------------------|-----------|-----------|-----|-------------|
| That is a happy person      | -0.0203 | -0.0104 | ... | -0.0053 |
| That is a happy dog         | -0.0196 | -0.0323 | ... | -0.0274 |
| That is a very happy person | -0.0136 | 0.0043  | ... | -0.0054 |
| Today is a sunny day        | -0.0075 | -0.0023 | ... | 0.0078  |
:::

On peut tester le modÃ¨le [ici](https://huggingface.co/sentence-transformers/all-mpnet-base-v2).

::: notes
- On peut facilement tÃ©lÃ©charger le modÃ¨le Ã  partir du site HuggingFace en utilisant la librairie Â« sentence_transformers Â».
- Le modÃ¨le qu'on utilise n'est pas exactement le SBERT original, mais quelque chose de trÃ¨s similaire. C'est le modÃ¨le de base, donc celui avec 12 blocs transformers et 110 millions de paramÃ¨tres.
- On peut commencer Ã  l'utiliser directement pour obtenir des embeddings de phrases avec la mÃ©thode Â« encode Â».
:::


## Imagerie {visibility="uncounted"}

Estimer le coÃ»t de reconstruction des maisons.

![](images/imagerie.mp4){width=950}

[PrÃ©cision de +/- 50k$]{style='margin-left:60px;'}

## Cartes d'inondations {visibility="uncounted"}

<br>

AmÃ©liorer les cartes d'inondation fournies par JBL, car celles-ci Â« pensaient Â» que les forÃªt sont des blocs qui ne laissent pas passer l'eau. 

::: colums

::: {.column width='49%'}
Avant

![](images/avant.png)
:::

::: {.column width='49%'}
AprÃ¨s

![](images/apres.png)
:::

:::


## Autres projets {visibility="uncounted"}

<br>

- DÃ©velopper un modÃ¨le de tarification tÃ©lÃ©matique pour le nouveau produit d'assurance tÃ©lÃ©matique.
- DÃ©velopper une application Shiny se basant sur le simulateur d'innondations pour mieux gÃ©rer notre risque de concentration.
- AmÃ©liorer le processus des indiquÃ©s en utilisant la thÃ©orie des valeurs extrÃªmes pour estimer une pÃ©riode de retour pour les grosses rÃ©clamations.
- Â« Fuzzy matching Â» pour l'Ã©quipe d'analytique de fraudes.


## Les dÃ©fis de l'industrie {visibility="uncounted"}

<br>
**Dans aucun ordre particulier:**

- Fraude
- Catastrophes naturelles: inondations, sÃ©ismes, feux de forÃªt, etc.
- ModÃ©lisation des grosses pertes
- Assurance tÃ©lÃ©matique
- Traitement de langage naturel
- Etc.

![](images/icon_flood.png){.absolute width=400 bottom=0 right='10%'}
